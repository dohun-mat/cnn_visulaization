{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ckYTSlivYn_",
        "outputId": "da9565e5-9a2b-43a6-9a88-fe60585e57ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Colab Notebooks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVjvSPXs2z0i",
        "outputId": "9ff29eb8-b76a-4b3c-a3f8-3d23522f8b67"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import copy\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.cm as mpl_color_map\n",
        "from matplotlib.colors import ListedColormap\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torchvision import models\n",
        "\n",
        "\n",
        "def convert_to_grayscale(im_as_arr):\n",
        "    \"\"\"\n",
        "        Converts 3d image to grayscale\n",
        "\n",
        "    Args:\n",
        "        im_as_arr (numpy arr): RGB image with shape (D,W,H)\n",
        "\n",
        "    returns:\n",
        "        grayscale_im (numpy_arr): Grayscale image with shape (1,W,D)\n",
        "    \"\"\"\n",
        "    grayscale_im = np.sum(np.abs(im_as_arr), axis=0)\n",
        "    im_max = np.percentile(grayscale_im, 99)\n",
        "    im_min = np.min(grayscale_im)\n",
        "    grayscale_im = (np.clip((grayscale_im - im_min) / (im_max - im_min), 0, 1))\n",
        "    grayscale_im = np.expand_dims(grayscale_im, axis=0)\n",
        "    return grayscale_im\n",
        "\n",
        "\n",
        "def save_gradient_images(gradient, file_name):\n",
        "    \"\"\"\n",
        "        Exports the original gradient image\n",
        "\n",
        "    Args:\n",
        "        gradient (np arr): Numpy array of the gradient with shape (3, 224, 224)\n",
        "        file_name (str): File name to be exported\n",
        "    \"\"\"\n",
        "    if not os.path.exists('../results'):\n",
        "        os.makedirs('../results')\n",
        "    # Normalize\n",
        "    gradient = gradient - gradient.min()\n",
        "    gradient /= gradient.max()\n",
        "    # Save image\n",
        "    path_to_file = os.path.join('../results', file_name + '.png')\n",
        "    save_image(gradient, path_to_file)\n",
        "\n",
        "\n",
        "def save_class_activation_images(org_img, activation_map, file_name):\n",
        "    \"\"\"\n",
        "        Saves cam activation map and activation map on the original image\n",
        "\n",
        "    Args:\n",
        "        org_img (PIL img): Original image\n",
        "        activation_map (numpy arr): Activation map (grayscale) 0-255\n",
        "        file_name (str): File name of the exported image\n",
        "    \"\"\"\n",
        "    if not os.path.exists('../results'):\n",
        "        os.makedirs('../results')\n",
        "    # Grayscale activation map\n",
        "    heatmap, heatmap_on_image = apply_colormap_on_image(org_img, activation_map, 'hsv')\n",
        "    # Save colored heatmap\n",
        "    path_to_file = os.path.join('../results', file_name+'_Cam_Heatmap.png')\n",
        "    save_image(heatmap, path_to_file)\n",
        "    # Save heatmap on iamge\n",
        "    path_to_file = os.path.join('../results', file_name+'_Cam_On_Image.png')\n",
        "    save_image(heatmap_on_image, path_to_file)\n",
        "    # SAve grayscale heatmap\n",
        "    path_to_file = os.path.join('../results', file_name+'_Cam_Grayscale.png')\n",
        "    save_image(activation_map, path_to_file)\n",
        "\n",
        "\n",
        "def apply_colormap_on_image(org_im, activation, colormap_name):\n",
        "    \"\"\"\n",
        "        Apply heatmap on image\n",
        "    Args:\n",
        "        org_img (PIL img): Original image\n",
        "        activation_map (numpy arr): Activation map (grayscale) 0-255\n",
        "        colormap_name (str): Name of the colormap\n",
        "    \"\"\"\n",
        "    # Get colormap\n",
        "    color_map = mpl_color_map.get_cmap(colormap_name)\n",
        "    no_trans_heatmap = color_map(activation)\n",
        "    # Change alpha channel in colormap to make sure original image is displayed\n",
        "    heatmap = copy.copy(no_trans_heatmap)\n",
        "    heatmap[:, :, 3] = 0.4\n",
        "    heatmap = Image.fromarray((heatmap*255).astype(np.uint8))\n",
        "    no_trans_heatmap = Image.fromarray((no_trans_heatmap*255).astype(np.uint8))\n",
        "\n",
        "    # Apply heatmap on image\n",
        "    heatmap_on_image = Image.new(\"RGBA\", org_im.size)\n",
        "    heatmap_on_image = Image.alpha_composite(heatmap_on_image, org_im.convert('RGBA'))\n",
        "    heatmap_on_image = Image.alpha_composite(heatmap_on_image, heatmap)\n",
        "    return no_trans_heatmap, heatmap_on_image\n",
        "\n",
        "\n",
        "def apply_heatmap(R, sx, sy):\n",
        "    \"\"\"\n",
        "        Heatmap code stolen from https://git.tu-berlin.de/gmontavon/lrp-tutorial\n",
        "\n",
        "        This is (so far) only used for LRP\n",
        "    \"\"\"\n",
        "    b = 10*((np.abs(R)**3.0).mean()**(1.0/3))\n",
        "    my_cmap = plt.cm.seismic(np.arange(plt.cm.seismic.N))\n",
        "    my_cmap[:, 0:3] *= 0.85\n",
        "    my_cmap = ListedColormap(my_cmap)\n",
        "    plt.figure(figsize=(sx, sy))\n",
        "    plt.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
        "    plt.axis('off')\n",
        "    heatmap = plt.imshow(R, cmap=my_cmap, vmin=-b, vmax=b, interpolation='nearest')\n",
        "    return heatmap\n",
        "    # plt.show()\n",
        "\n",
        "\n",
        "def format_np_output(np_arr):\n",
        "    \"\"\"\n",
        "        This is a (kind of) bandaid fix to streamline saving procedure.\n",
        "        It converts all the outputs to the same format which is 3xWxH\n",
        "        with using sucecssive if clauses.\n",
        "    Args:\n",
        "        im_as_arr (Numpy array): Matrix of shape 1xWxH or WxH or 3xWxH\n",
        "    \"\"\"\n",
        "    # Phase/Case 1: The np arr only has 2 dimensions\n",
        "    # Result: Add a dimension at the beginning\n",
        "    if len(np_arr.shape) == 2:\n",
        "        np_arr = np.expand_dims(np_arr, axis=0)\n",
        "    # Phase/Case 2: Np arr has only 1 channel (assuming first dim is channel)\n",
        "    # Result: Repeat first channel and convert 1xWxH to 3xWxH\n",
        "    if np_arr.shape[0] == 1:\n",
        "        np_arr = np.repeat(np_arr, 3, axis=0)\n",
        "    # Phase/Case 3: Np arr is of shape 3xWxH\n",
        "    # Result: Convert it to WxHx3 in order to make it saveable by PIL\n",
        "    if np_arr.shape[0] == 3:\n",
        "        np_arr = np_arr.transpose(1, 2, 0)\n",
        "    # Phase/Case 4: NP arr is normalized between 0-1\n",
        "    # Result: Multiply with 255 and change type to make it saveable by PIL\n",
        "    if np.max(np_arr) <= 1:\n",
        "        np_arr = (np_arr*255).astype(np.uint8)\n",
        "    return np_arr\n",
        "\n",
        "\n",
        "def save_image(im, path):\n",
        "    \"\"\"\n",
        "        Saves a numpy matrix or PIL image as an image\n",
        "    Args:\n",
        "        im_as_arr (Numpy array): Matrix of shape DxWxH\n",
        "        path (str): Path to the image\n",
        "    \"\"\"\n",
        "    if isinstance(im, (np.ndarray, np.generic)):\n",
        "        im = format_np_output(im)\n",
        "        im = Image.fromarray(im)\n",
        "    im.save(path)\n",
        "\n",
        "\n",
        "def preprocess_image(pil_im, resize_im=True):\n",
        "    \"\"\"\n",
        "        Processes image for CNNs\n",
        "\n",
        "    Args:\n",
        "        PIL_img (PIL_img): PIL Image or numpy array to process\n",
        "        resize_im (bool): Resize to 224 or not\n",
        "    returns:\n",
        "        im_as_var (torch variable): Variable that contains processed float tensor\n",
        "    \"\"\"\n",
        "    # Mean and std list for channels (Imagenet)\n",
        "    mean = [0.485, 0.456, 0.406]\n",
        "    std = [0.229, 0.224, 0.225]\n",
        "\n",
        "    # Ensure or transform incoming image to PIL image\n",
        "    if type(pil_im) != Image.Image:\n",
        "        try:\n",
        "            pil_im = Image.fromarray(pil_im)\n",
        "        except Exception as e:\n",
        "            print(\"could not transform PIL_img to a PIL Image object. Please check input.\")\n",
        "\n",
        "    # Resize image\n",
        "    if resize_im:\n",
        "        pil_im = pil_im.resize((224, 224), Image.ANTIALIAS)\n",
        "\n",
        "    im_as_arr = np.float32(pil_im)\n",
        "    im_as_arr = im_as_arr.transpose(2, 0, 1)  # Convert array to D,W,H\n",
        "    # Normalize the channels\n",
        "    for channel, _ in enumerate(im_as_arr):\n",
        "        im_as_arr[channel] /= 255\n",
        "        im_as_arr[channel] -= mean[channel]\n",
        "        im_as_arr[channel] /= std[channel]\n",
        "    # Convert to float tensor\n",
        "    im_as_ten = torch.from_numpy(im_as_arr).float()\n",
        "    # Add one more channel to the beginning. Tensor shape = 1,3,224,224\n",
        "    im_as_ten.unsqueeze_(0)\n",
        "    # Convert to Pytorch variable\n",
        "    im_as_var = Variable(im_as_ten, requires_grad=True)\n",
        "    return im_as_var\n",
        "\n",
        "\n",
        "def recreate_image(im_as_var):\n",
        "    \"\"\"\n",
        "        Recreates images from a torch variable, sort of reverse preprocessing\n",
        "    Args:\n",
        "        im_as_var (torch variable): Image to recreate\n",
        "    returns:\n",
        "        recreated_im (numpy arr): Recreated image in array\n",
        "    \"\"\"\n",
        "    reverse_mean = [-0.485, -0.456, -0.406]\n",
        "    reverse_std = [1/0.229, 1/0.224, 1/0.225]\n",
        "    recreated_im = copy.copy(im_as_var.data.numpy()[0])\n",
        "    for c in range(3):\n",
        "        recreated_im[c] /= reverse_std[c]\n",
        "        recreated_im[c] -= reverse_mean[c]\n",
        "    recreated_im[recreated_im > 1] = 1\n",
        "    recreated_im[recreated_im < 0] = 0\n",
        "    recreated_im = np.round(recreated_im * 255)\n",
        "\n",
        "    recreated_im = np.uint8(recreated_im).transpose(1, 2, 0)\n",
        "    return recreated_im\n",
        "\n",
        "\n",
        "def get_positive_negative_saliency(gradient):\n",
        "    \"\"\"\n",
        "        Generates positive and negative saliency maps based on the gradient\n",
        "    Args:\n",
        "        gradient (numpy arr): Gradient of the operation to visualize\n",
        "\n",
        "    returns:\n",
        "        pos_saliency ( )\n",
        "    \"\"\"\n",
        "    pos_saliency = (np.maximum(0, gradient) / gradient.max())\n",
        "    neg_saliency = (np.maximum(0, -gradient) / -gradient.min())\n",
        "    return pos_saliency, neg_saliency\n",
        "\n",
        "\n",
        "def get_example_params(example_index):\n",
        "    \"\"\"\n",
        "        Gets used variables for almost all visualizations, like the image, model etc.\n",
        "\n",
        "    Args:\n",
        "        example_index (int): Image id to use from examples\n",
        "\n",
        "    returns:\n",
        "        original_image (numpy arr): Original image read from the file\n",
        "        prep_img (numpy_arr): Processed image\n",
        "        target_class (int): Target class for the image\n",
        "        file_name_to_export (string): File name to export the visualizations\n",
        "        pretrained_model(Pytorch model): Model to use for the operations\n",
        "    \"\"\"\n",
        "    # Pick one of the examples\n",
        "    example_list = (('lion.jpg', 56),\n",
        "                    # ('../input_images/cat_dog.png', 243),\n",
        "                    # ('../input_images/spider.png', 72)\n",
        "                    )\n",
        "    img_path = example_list[example_index][0]\n",
        "    target_class = example_list[example_index][1]\n",
        "    file_name_to_export = img_path[img_path.rfind('/')+1:img_path.rfind('.')]\n",
        "    # Read image\n",
        "    original_image = Image.open(img_path).convert('RGB')\n",
        "    # Process image\n",
        "    prep_img = preprocess_image(original_image)\n",
        "    # Define model\n",
        "    pretrained_model = models.alexnet(pretrained=True)\n",
        "    return (original_image,\n",
        "            prep_img,\n",
        "            target_class,\n",
        "            file_name_to_export,\n",
        "            pretrained_model)"
      ],
      "metadata": {
        "id": "ElP-HzxX2hD3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import ReLU\n",
        "\n",
        "# from misc_functions import (get_example_params,\n",
        "#                             convert_to_grayscale,\n",
        "#                             save_gradient_images,\n",
        "#                             get_positive_negative_saliency)\n",
        "\n",
        "\n",
        "class GuidedBackprop():\n",
        "    \"\"\"\n",
        "       Produces gradients generated with guided back propagation from the given image\n",
        "    \"\"\"\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.gradients = None\n",
        "        self.forward_relu_outputs = []\n",
        "        # Put model in evaluation mode\n",
        "        self.model.eval()\n",
        "        self.update_relus()\n",
        "        self.hook_layers()\n",
        "\n",
        "    def hook_layers(self):\n",
        "        def hook_function(module, grad_in, grad_out):\n",
        "            self.gradients = grad_in[0]\n",
        "        # Register hook to the first layer\n",
        "        first_layer = list(self.model.features._modules.items())[0][1]\n",
        "        first_layer.register_backward_hook(hook_function)\n",
        "\n",
        "    def update_relus(self):\n",
        "        \"\"\"\n",
        "            Updates relu activation functions so that\n",
        "                1- stores output in forward pass\n",
        "                2- imputes zero for gradient values that are less than zero\n",
        "        \"\"\"\n",
        "        def relu_backward_hook_function(module, grad_in, grad_out):\n",
        "            \"\"\"\n",
        "            If there is a negative gradient, change it to zero\n",
        "            \"\"\"\n",
        "            # Get last forward output\n",
        "            corresponding_forward_output = self.forward_relu_outputs[-1]\n",
        "            corresponding_forward_output[corresponding_forward_output > 0] = 1\n",
        "            modified_grad_out = corresponding_forward_output * torch.clamp(grad_in[0], min=0.0)\n",
        "            del self.forward_relu_outputs[-1]  # Remove last forward output\n",
        "            return (modified_grad_out,)\n",
        "\n",
        "        def relu_forward_hook_function(module, ten_in, ten_out):\n",
        "            \"\"\"\n",
        "            Store results of forward pass\n",
        "            \"\"\"\n",
        "            self.forward_relu_outputs.append(ten_out)\n",
        "\n",
        "        # Loop through layers, hook up ReLUs\n",
        "        for pos, module in self.model.features._modules.items():\n",
        "            if isinstance(module, ReLU):\n",
        "                module.register_backward_hook(relu_backward_hook_function)\n",
        "                module.register_forward_hook(relu_forward_hook_function)\n",
        "\n",
        "    def generate_gradients(self, input_image, target_class):\n",
        "        # Forward pass\n",
        "        model_output = self.model(input_image)\n",
        "        # Zero gradients\n",
        "        self.model.zero_grad()\n",
        "        # Target for backprop\n",
        "        one_hot_output = torch.FloatTensor(1, model_output.size()[-1]).zero_()\n",
        "        one_hot_output[0][target_class] = 1\n",
        "        # Backward pass\n",
        "        model_output.backward(gradient=one_hot_output)\n",
        "        # Convert Pytorch variable to numpy array\n",
        "        # [0] to get rid of the first channel (1,3,224,224)\n",
        "        gradients_as_arr = self.gradients.data.numpy()[0]\n",
        "        return gradients_as_arr\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    target_example = 0  # Snake\n",
        "    (original_image, prep_img, target_class, file_name_to_export, pretrained_model) =get_example_params(target_example)\n",
        "\n",
        "    # Guided backprop\n",
        "    GBP = GuidedBackprop(pretrained_model)\n",
        "    # Get gradients\n",
        "    guided_grads = GBP.generate_gradients(prep_img, target_class)\n",
        "    # Save colored gradients\n",
        "    save_gradient_images(guided_grads, file_name_to_export + '_Guided_BP_color')\n",
        "    # Convert to grayscale\n",
        "    grayscale_guided_grads = convert_to_grayscale(guided_grads)\n",
        "    # Save grayscale gradients\n",
        "    save_gradient_images(grayscale_guided_grads, file_name_to_export + '_Guided_BP_gray')\n",
        "    # Positive and negative saliency maps\n",
        "    pos_sal, neg_sal = get_positive_negative_saliency(guided_grads)\n",
        "    save_gradient_images(pos_sal, file_name_to_export + '_pos_sal')\n",
        "    save_gradient_images(neg_sal, file_name_to_export + '_neg_sal')\n",
        "    print('Guided backprop completed')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNcURESwwHom",
        "outputId": "7611258f-facc-4c2f-abb2-7bb88074a5c3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-c4a2e4f0941f>:179: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
            "  pil_im = pil_im.resize((224, 224), Image.ANTIALIAS)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
            "100%|██████████| 233M/233M [00:02<00:00, 99.1MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Guided backprop completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TS8Xqlrd2-3F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}