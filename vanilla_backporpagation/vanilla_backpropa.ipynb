{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uc0PYcOssKN6",
        "outputId": "7a91573c-3da0-43dc-e704-95e7ecf9f43f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Colab Notebooks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ERj_XBsvkw8",
        "outputId": "cde0ef95-6de2-4b3e-bc04-ff6f1da4dfc0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "from torchvision import models"
      ],
      "metadata": {
        "id": "QZfeav1-ugGW"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(pil_im, resize_im=True):\n",
        "    \"\"\"\n",
        "        Processes image for CNNs\n",
        "\n",
        "    Args:\n",
        "        PIL_img (PIL_img): PIL Image or numpy array to process\n",
        "        resize_im (bool): Resize to 224 or not\n",
        "    returns:\n",
        "        im_as_var (torch variable): Variable that contains processed float tensor\n",
        "    \"\"\"\n",
        "    # Mean and std list for channels (Imagenet)\n",
        "    mean = [0.485, 0.456, 0.406]\n",
        "    std = [0.229, 0.224, 0.225]\n",
        "\n",
        "    # Ensure or transform incoming image to PIL image\n",
        "    if type(pil_im) != Image.Image:\n",
        "        try:\n",
        "            pil_im = Image.fromarray(pil_im)\n",
        "        except Exception as e:\n",
        "            print(\"could not transform PIL_img to a PIL Image object. Please check input.\")\n",
        "\n",
        "    # Resize image\n",
        "    if resize_im:\n",
        "        pil_im = pil_im.resize((224, 224), Image.ANTIALIAS)\n",
        "\n",
        "    im_as_arr = np.float32(pil_im)\n",
        "    im_as_arr = im_as_arr.transpose(2, 0, 1)  # Convert array to D,W,H\n",
        "    # Normalize the channels\n",
        "    for channel, _ in enumerate(im_as_arr):\n",
        "        im_as_arr[channel] /= 255\n",
        "        im_as_arr[channel] -= mean[channel]\n",
        "        im_as_arr[channel] /= std[channel]\n",
        "    # Convert to float tensor\n",
        "    im_as_ten = torch.from_numpy(im_as_arr).float()\n",
        "    # Add one more channel to the beginning. Tensor shape = 1,3,224,224\n",
        "    im_as_ten.unsqueeze_(0)\n",
        "    # Convert to Pytorch variable\n",
        "    im_as_var = Variable(im_as_ten, requires_grad=True)\n",
        "    return im_as_var"
      ],
      "metadata": {
        "id": "6fpZcBuzuMfd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_example_params(example_index):\n",
        "    \"\"\"\n",
        "        Gets used variables for almost all visualizations, like the image, model etc.\n",
        "\n",
        "    Args:\n",
        "        example_index (int): Image id to use from examples\n",
        "\n",
        "    returns:\n",
        "        original_image (numpy arr): Original image read from the file\n",
        "        prep_img (numpy_arr): Processed image\n",
        "        target_class (int): Target class for the image\n",
        "        file_name_to_export (string): File name to export the visualizations\n",
        "        pretrained_model(Pytorch model): Model to use for the operations\n",
        "    \"\"\"\n",
        "    # Pick one of the examples\n",
        "    example_list = (('lion.jpg', 56),\n",
        "                    # ('../input_images/cat_dog.png', 243),\n",
        "                    # ('../input_images/spider.png', 72)\n",
        "                    )\n",
        "    img_path = example_list[example_index][0]\n",
        "    target_class = example_list[example_index][1]\n",
        "    file_name_to_export = img_path[img_path.rfind('/')+1:img_path.rfind('.')]\n",
        "    # Read image\n",
        "    original_image = Image.open(img_path).convert('RGB')\n",
        "    # Process image\n",
        "    prep_img = preprocess_image(original_image)\n",
        "    # Define model\n",
        "    pretrained_model = models.alexnet(pretrained=True)\n",
        "    return (original_image,\n",
        "            prep_img,\n",
        "            target_class,\n",
        "            file_name_to_export,\n",
        "            pretrained_model)"
      ],
      "metadata": {
        "id": "-CH_-qwTtw-e"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_grayscale(im_as_arr):\n",
        "    \"\"\"\n",
        "        Converts 3d image to grayscale\n",
        "\n",
        "    Args:\n",
        "        im_as_arr (numpy arr): RGB image with shape (D,W,H)\n",
        "\n",
        "    returns:\n",
        "        grayscale_im (numpy_arr): Grayscale image with shape (1,W,D)\n",
        "    \"\"\"\n",
        "    grayscale_im = np.sum(np.abs(im_as_arr), axis=0)\n",
        "    im_max = np.percentile(grayscale_im, 99)\n",
        "    im_min = np.min(grayscale_im)\n",
        "    grayscale_im = (np.clip((grayscale_im - im_min) / (im_max - im_min), 0, 1))\n",
        "    grayscale_im = np.expand_dims(grayscale_im, axis=0)\n",
        "    return grayscale_im"
      ],
      "metadata": {
        "id": "sLfg7Wvgt_PJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_gradient_images(gradient, file_name):\n",
        "    \"\"\"\n",
        "        Exports the original gradient image\n",
        "\n",
        "    Args:\n",
        "        gradient (np arr): Numpy array of the gradient with shape (3, 224, 224)\n",
        "        file_name (str): File name to be exported\n",
        "    \"\"\"\n",
        "    if not os.path.exists('../results'):\n",
        "        os.makedirs('../results')\n",
        "    # Normalize\n",
        "    gradient = gradient - gradient.min()\n",
        "    gradient /= gradient.max()\n",
        "    # Save image\n",
        "    path_to_file = os.path.join('../results', file_name + '.png')\n",
        "    save_image(gradient, path_to_file)"
      ],
      "metadata": {
        "id": "vbcNsop0uT_E"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_np_output(np_arr):\n",
        "    \"\"\"\n",
        "        This is a (kind of) bandaid fix to streamline saving procedure.\n",
        "        It converts all the outputs to the same format which is 3xWxH\n",
        "        with using sucecssive if clauses.\n",
        "    Args:\n",
        "        im_as_arr (Numpy array): Matrix of shape 1xWxH or WxH or 3xWxH\n",
        "    \"\"\"\n",
        "    # Phase/Case 1: The np arr only has 2 dimensions\n",
        "    # Result: Add a dimension at the beginning\n",
        "    if len(np_arr.shape) == 2:\n",
        "        np_arr = np.expand_dims(np_arr, axis=0)\n",
        "    # Phase/Case 2: Np arr has only 1 channel (assuming first dim is channel)\n",
        "    # Result: Repeat first channel and convert 1xWxH to 3xWxH\n",
        "    if np_arr.shape[0] == 1:\n",
        "        np_arr = np.repeat(np_arr, 3, axis=0)\n",
        "    # Phase/Case 3: Np arr is of shape 3xWxH\n",
        "    # Result: Convert it to WxHx3 in order to make it saveable by PIL\n",
        "    if np_arr.shape[0] == 3:\n",
        "        np_arr = np_arr.transpose(1, 2, 0)\n",
        "    # Phase/Case 4: NP arr is normalized between 0-1\n",
        "    # Result: Multiply with 255 and change type to make it saveable by PIL\n",
        "    if np.max(np_arr) <= 1:\n",
        "        np_arr = (np_arr*255).astype(np.uint8)\n",
        "    return np_arr"
      ],
      "metadata": {
        "id": "3J9_6BdBweQM"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_image(im, path):\n",
        "    \"\"\"\n",
        "        Saves a numpy matrix or PIL image as an image\n",
        "    Args:\n",
        "        im_as_arr (Numpy array): Matrix of shape DxWxH\n",
        "        path (str): Path to the image\n",
        "    \"\"\"\n",
        "    if isinstance(im, (np.ndarray, np.generic)):\n",
        "        im = format_np_output(im)\n",
        "        im = Image.fromarray(im)\n",
        "    im.save(path)"
      ],
      "metadata": {
        "id": "j2jAYqSkwWx-"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# from misc_functions import get_example_params, convert_to_grayscale, save_gradient_images\n",
        "\n",
        "\n",
        "class VanillaBackprop():\n",
        "    \"\"\"\n",
        "        Produces gradients generated with vanilla back propagation from the image\n",
        "    \"\"\"\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.gradients = None\n",
        "        # Put model in evaluation mode\n",
        "        self.model.eval()\n",
        "        # Hook the first layer to get the gradient\n",
        "        self.hook_layers()\n",
        "\n",
        "    def hook_layers(self):\n",
        "        def hook_function(module, grad_in, grad_out):\n",
        "            self.gradients = grad_in[0]\n",
        "\n",
        "        # Register hook to the first layer\n",
        "        first_layer = list(self.model.features._modules.items())[0][1]\n",
        "        first_layer.register_backward_hook(hook_function)\n",
        "\n",
        "    def generate_gradients(self, input_image, target_class):\n",
        "        # Forward\n",
        "        model_output = self.model(input_image)\n",
        "        # Zero grads\n",
        "        self.model.zero_grad()\n",
        "        # Target for backprop\n",
        "        one_hot_output = torch.FloatTensor(1, model_output.size()[-1]).zero_()\n",
        "        one_hot_output[0][target_class] = 1\n",
        "        # Backward pass\n",
        "        model_output.backward(gradient=one_hot_output)\n",
        "        # Convert Pytorch variable to numpy array\n",
        "        # [0] to get rid of the first channel (1,3,224,224)\n",
        "        gradients_as_arr = self.gradients.data.numpy()[0]\n",
        "        return gradients_as_arr"
      ],
      "metadata": {
        "id": "kDXTWPHRsRf0"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # Get params\n",
        "    target_example = 0  # Snake\n",
        "    (original_image, prep_img, target_class, file_name_to_export, pretrained_model) =get_example_params(target_example)\n",
        "    # Vanilla backprop\n",
        "    VBP = VanillaBackprop(pretrained_model)\n",
        "    # Generate gradients\n",
        "    vanilla_grads = VBP.generate_gradients(prep_img, target_class)\n",
        "    # Save colored gradients\n",
        "    save_gradient_images(vanilla_grads, file_name_to_export + '_Vanilla_BP_color')\n",
        "    # Convert to grayscale\n",
        "    grayscale_vanilla_grads = convert_to_grayscale(vanilla_grads)\n",
        "    # Save grayscale gradients\n",
        "    save_gradient_images(grayscale_vanilla_grads, file_name_to_export + '_Vanilla_BP_gray')\n",
        "    print('Vanilla backprop completed')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEWePqJ3sgqu",
        "outputId": "404ece58-47e6-42c6-cdd6-f44dc6203955"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-ce4fb77c7bcd>:24: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
            "  pil_im = pil_im.resize((224, 224), Image.ANTIALIAS)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vanilla backprop completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0sl-83Bjsg28"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}